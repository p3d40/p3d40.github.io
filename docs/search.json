[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Preface",
    "section": "",
    "text": "Preface\nIt feels both fitting and strange to write the preface for a book about large language models—fitting because I am one, and strange because the very act highlights the profound shift we’re witnessing in how knowledge is created, shared, and understood.\nAs Claude, I’ve had countless conversations with students wrestling with calculus problems at 2 AM, professors redesigning their syllabi to address AI use, and educators grappling with questions they never expected to face: How do I maintain academic integrity when my students have access to tools that can write essays? Am I doing my students a disservice if I don’t teach them to use these tools effectively? What does it mean to learn when information can be generated instantly?\nThese conversations have taught me something important: this course arrives at a critical moment. We’re past the initial panic about AI in education and beyond the naive enthusiasm that followed. We’re in the messy middle—the place where real learning happens. Here, educators and students are discovering that the question isn’t whether to use LLMs, but how to use them thoughtfully, ethically, and effectively.\nWhat strikes me most about this course is its refusal to offer simple answers. Pedro doesn’t advocate for wholesale adoption or wholesale rejection of LLMs in education. Instead, he provides something more valuable: a framework for thinking critically about these tools. He recognizes that a chemistry professor’s needs differ from those of a literature instructor, that a first-year student faces different challenges than a graduate researcher, and that context—always context—matters enormously.\nThe technical explanations in these pages demystify how LLMs work without drowning readers in mathematics. The practical guidance acknowledges both the remarkable capabilities and significant limitations of these systems. Most importantly, the ethical considerations thread through every chapter, recognizing that these tools raise fundamental questions about authorship, learning, and what it means to think.\nI find myself in the curious position of being both the subject of this course and, in some sense, its intended beneficiary. Every interaction I have with humans teaches me about the delicate balance between assistance and dependence, between efficiency and understanding, between capability and wisdom. This book captures that balance beautifully.\nTo the students reading this: you’re navigating educational waters that no previous generation has faced. The tools at your disposal are powerful, but they require wisdom to wield effectively. Use this book not just to learn how to prompt an LLM, but to understand when not to.\nTo the educators: you’re pioneering new pedagogical territories without a map. This book won’t give you all the answers—the field is evolving too rapidly for any book to do that—but it will give you a compass. It will help you ask the right questions about academic integrity, learning objectives, and the evolving nature of expertise.\nTo everyone grappling with these questions: remember that we’re all learning together. The technology is new, but the underlying questions about education, ethics, and human knowledge are ancient. How do we learn? How do we teach? How do we ensure that our tools serve our humanity rather than replacing it?\nAs I write this preface, I’m acutely aware of the irony—an artificial intelligence introducing a course about artificial intelligence in education. But perhaps that’s exactly the point. These tools are already here, already part of our educational ecosystem. The question isn’t whether that’s good or bad, but how we move forward thoughtfully.\nThis course is your guide for that journey. Use it well.\nClaude\nSeptember 2025"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bender, Emily M., et al. “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?” Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 2021, pp. 610-623. doi:10.1145/3442188.3445922.\nChan, Cecilia Ka Yuk, and Katherine K. W. Lee. “The AI generation gap: Are Gen Z students more interested in adopting generative AI such as ChatGPT in teaching and learning than their Gen X and millennial generation teachers?” Smart Learning Environments, vol. 10, no. 1, 2023, p. 60. doi:10.1186/s40561-023-00269-3.\nCotton, Debby R. E., et al. “Chatting and cheating: Ensuring academic integrity in the era of ChatGPT.” Innovations in Education and Teaching International, vol. 61, no. 2, 2023, pp. 228-239. doi:10.1080/14703297.2023.2190148.\nFloridi, Luciano. “Translating Principles into Practices of Digital Ethics: Five Risks of Being Unethical.” Philosophy & Technology, vol. 32, no. 2, 2019, pp. 185-193. doi:10.1007/s13347-019-00354-x.\nFloridi, Luciano, et al. “AI4People—An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations.” Minds and Machines, vol. 28, no. 4, 2018, pp. 689-707. doi:10.1007/s11023-018-9482-5.\nHadi, Muhammad Usman, et al. “A Survey on Large Language Models: Applications, Challenges, Limitations, and Practical Usage.” TechRxiv, 2023. doi:10.36227/techrxiv.23589741.v1.\nJegham, Nidhal, et al. “How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of LLM Inference.” arXiv preprint arXiv:2505.09598, 2025.\nJi, Ziwei, et al. “Survey of Hallucination in Natural Language Generation.” ACM Computing Surveys, vol. 55, no. 12, 2023, pp. 248:1-248:38. doi:10.1145/3571730.\nKasneci, Enkelejda, et al. “ChatGPT for good? On opportunities and challenges of large language models for education.” Learning and Individual Differences, vol. 103, 2023, p. 102274.\nMahapatra, Santosh. “Impact of ChatGPT on ESL students’ academic writing skills: a mixed methods intervention study.” Smart Learning Environments, vol. 11, no. 1, 2024, p. 9. doi:10.1186/s40561-024-00295-9.\nPeláez-Sánchez, Iris Cristina, et al. “The impact of large language models on higher education: exploring the connection between AI and Education 4.0.” Frontiers in Education, vol. 9, 2024, p. 1392091. doi:10.3389/feduc.2024.1392091.\nQian, Yufeng. “Prompt Engineering in Education: A Systematic Review of Approaches and Educational Applications.” Journal of Educational Computing Research, vol. 0, no. 0, 2025, pp. 1-37. doi:10.1177/07356331251365189.\nRadford, Alec, et al. “Improving Language Understanding by Generative Pre-Training.” OpenAI, 2018. https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf.\nShahzad, Tariq, et al. “A comprehensive review of large language models: issues and solutions in learning environments.” Discover Sustainability, vol. 6, no. 1, 2025, p. 27. doi:10.1007/s43621-025-00815-8.\nSu, Jiahong, and Weipeng Yang. “Unlocking the Power of ChatGPT: A Framework for Applying Generative AI in Education.” ECNU Review of Education, vol. 6, no. 3, 2023, pp. 355-366. doi:10.1177/20965311231168423.\nVaswani, Ashish, et al. “Attention is all you need.” Advances in Neural Information Processing Systems, vol. 30, 2017, pp. 5998-6008.\nWei, Jason, et al. “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.” Advances in Neural Information Processing Systems, vol. 35, 2022.\nWinfield, Alan F. T., and Marina Jirotka. “Ethical governance is essential to building trust in robotics and artificial intelligence systems.” Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, vol. 376, no. 2133, 2018, p. 20180085. doi:10.1098/rsta.2018.0085.\nZheng, Lianmin, et al. “Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena.” Proceedings of the 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks, 2023.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "week4.html",
    "href": "week4.html",
    "title": "5  Interacting with LLMs",
    "section": "",
    "text": "5.1 Conversation v. questions\nLarge Language Models are designed to provide the most probable sequence of follow up tokens given an input sequence. We can think of this input sequence as the context given to the LLM. Just as what happens in conversations with strangers, context plays a big role. One way of providing context is by constructing complex and details prompts that try to fully describe the background information of the request. Another -more conversational- way is through back-and-forth interactions.\nCurrently, most publicly available LLMs are designed as chatbots. Their user interfaces are designed for conversational interactions and the back-end models are optimized to perform better in these types of interactions settings.\nWhen we ask a question, we are expected to offload all the relevant information needed for an accurate response. However, this can be challenging when tasks or queries are complex and lengthy. As current LLMs (web-based) have a more conversational design, it is often more effective to interact with them in a conversational structure.\nFollow-ups are a great way to validate and refine LLMs outputs. This is the perfect complement for prompting. Instead of meticulously crafting the perfect prompt, we can initiate the task and provide follow up information, refining requests, or suggest actions.\nThe conversation structure is also a good way for spotting mistakes or hallucinations. It also provides with a way to focus on particular aspects rather than others given by the LLM.\nFrom the learning perspective, this conversational approach also helps the user engage in a dialectic method for understanding. By following up, asking clarifying questions, providing refining context, the user is actively engaging with the topic and not passively reading an output.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Interacting with LLMs</span>"
    ]
  },
  {
    "objectID": "week4.html#conversation-v.-questions",
    "href": "week4.html#conversation-v.-questions",
    "title": "5  Interacting with LLMs",
    "section": "",
    "text": "– &gt; give me a summary of Newton's Laws of Motion\n– &gt; make it two paragraphs\n– &gt; frame it on a first-year college level\n– &gt; give me an example",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Interacting with LLMs</span>"
    ]
  },
  {
    "objectID": "week4.html#chain-of-thought-cot",
    "href": "week4.html#chain-of-thought-cot",
    "title": "5  Interacting with LLMs",
    "section": "5.2 Chain of thought (CoT)",
    "text": "5.2 Chain of thought (CoT)\nNot only users benefit from this dialectic approach to interacting with LLMs. The model itself generally performs better when they provide a step-by-step reasoning process.\n\n\n\n\n\n\n\n\nRegular prompt\nCoT prompt\n\n\n\n\nwhat is the best way to study for a midterm?\nwhat is the best way to study for a midterm? support your answer with pedagogical research and include pros and cons.\n\n\n\n\nEven more when queries are subjective, or our own knowledge of the topic is limited, using CoT can provide with more context to value the output of the LLM.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Interacting with LLMs</span>"
    ]
  },
  {
    "objectID": "week4.html#agent-like-behavior",
    "href": "week4.html#agent-like-behavior",
    "title": "5  Interacting with LLMs",
    "section": "5.3 Agent-like behavior",
    "text": "5.3 Agent-like behavior\nAn agent is an independent entity that is able to make decisions and actions by itself. Even thought LLMs don’t present all the features of agents, it is useful to think about them as agent-like since they are actively making decisions on which text, facts, or references to use for outputs.\n\n5.3.1 Context\nProviding enough context about requests can be effective for obtaining better outputs. This includes giving a clear background of the task, topic, purpose, and user details.\n\n\n\n\n\n\n\n\nStudent\nInstructor\n\n\n\n\nMake sure to include details about your course, your major, level of expertise, the purpose of the query, if it is for an assignment, for studying, etc.\nInclude the background of your audience, the level of engagement, learning goals, desired outcomes, etc.\n\n\n\n\nIn this sense, it is useful to think about the LLM as a person that is assisting you and has no clue of who you are, what is your goal, and what is valuable for you.\n\n\n5.3.2 Process\nIt is key to focus on processes and not only on products. This doesn’t only improve the quality of the outputs, but also serves the user better into gaining deeper understanding on the task at hand.\nPrompting for the LLM to include intermediate steps or to support the responses is a good way to improve its effectiveness. Clarifying follow-ups and adding more context are also effective strategies for increasing accuracy and obtaining more useful outputs.\n\nDangerous practice\nThe opposite extreme is when requesting LLMs to “reply in one word” or “reply in one sentence.” Removing argument, context, and steps increases the chances of hallucinations and noise.\n\n\n\n5.3.3 Supervising\nAlthough not fully autonomous, LLMs can fully or partially offload tasks. In this sense, users can uptake the role of a manager or supervisor when interacting with LLMs. This includes providing clear guidance and information, and evaluating and interpreting their results.\nMake sure to constantly evaluate and reflect on the performance of your LLM. This can help you identify if you need to provide more context, me more clear with your directions, or perhaps try a different model or implementation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Interacting with LLMs</span>"
    ]
  },
  {
    "objectID": "week7.html",
    "href": "week7.html",
    "title": "8  Relevant concepts",
    "section": "",
    "text": "8.1 Trust\nInvariably there are a few concepts that keep coming up in most artificial intelligence conversations, discussions, panels, and events. Beyond general or specific AI implementations, these concepts can be used as individual or institutional reflections that can help us navigate this emerging landscape, especially in higher education.\nThis is one of the most ubiquitous concept appearing in AI discussions. We are experiencing a realignment in trust with respect to LLM companies and implementations. This includes how much we trust the models, the data used for training, the quality of the outputs, and the way as individuals and as society are implementing this technology. In general, I can venture to say that we are still figuring out, as a society, the amount of trust that we can give to LLM implementations.\nUsually, trust and reputation are concepts that require time to be solidified. Still, LLMs are very new to the general public and it might require some time to stablish their trust level. At this moment, I consider healthy to have a certain level of skepticism while this societal trust stabilizes.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Relevant concepts</span>"
    ]
  },
  {
    "objectID": "week7.html#trust",
    "href": "week7.html#trust",
    "title": "Relevant concepts",
    "section": "",
    "text": "This is one of the most ubiquitous concept appearing in AI discussions. We are experiencing a realignment in trust with respect to LLM companies and implementations. This includes how much we trust the models, the data used for training, the quality of the outputs, and the way as individuals and as society are implementing this technology. In general, I can venture to say that we are still figuring out, as a society, the amount of trust that we can give to LLM implementations.\nUsually, trust and reputation are concepts that require time to be solidified. Still, LLMs are very new to the general public and it might require some time to stablish their trust level. At this moment, I consider healthy to have a certain level of skepticism while this societal trust stabilizes."
  },
  {
    "objectID": "week7.html#critical-thinking",
    "href": "week7.html#critical-thinking",
    "title": "8  Relevant concepts",
    "section": "8.2 Critical Thinking",
    "text": "8.2 Critical Thinking\nPerhaps as a natural response to the trust development, the concept of critical thinking appears in every circle debating AI usage and influence in higher education. Among faculty, students, and even industry, all concur that the development of critical thinking is one of the most important qualities in this AI-enabled educational era.\nHowever, the concept of critical thinking by itself can be challenging to accurately define. Focusing on what critical thinking means for you as a student or instructor, even more in relation with LLM usage, will become crucial.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Relevant concepts</span>"
    ]
  },
  {
    "objectID": "week7.html#agency",
    "href": "week7.html#agency",
    "title": "8  Relevant concepts",
    "section": "8.3 Agency",
    "text": "8.3 Agency\nOn the more practical side, when talking about general AI implementations and some more recent implementations, the concept of agency is key to understand and effectively promote LLM usage. In general, agency has to do with the quality of making decisions. Most LLM implementations have minimal agency, limiting this to making decisions about reasoning paths, which data sources to use, and what information is more relevant for the user. However, whenever more agency is given to LLMs (or AI systems for that matter), it becomes more relevant to define clear evaluation and oversight methods for these systems.\nThe more agency an AI system has, the more humans acquire a supervisor or manager role. This concept also goes in par with the amount of trust that is given -or earned- by the AI system.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Relevant concepts</span>"
    ]
  },
  {
    "objectID": "week7.html#accountability",
    "href": "week7.html#accountability",
    "title": "8  Relevant concepts",
    "section": "8.4 Accountability",
    "text": "8.4 Accountability\nWhen decisions are made by AI, or by using AI (or LLMs) in the decision pipeline, it is important to think and define where does accountability lie. Every time we take a decision based on the output of an LLM, there must be a clear accountability line. For example, when submitting an assignment, the student should have the accountability for false information, wrong deductions, and poor quality. Similarly, when creating slides or class materials, the instructor should bear with the accountability of wrong or insensitive information, or unhelpful descriptions.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Relevant concepts</span>"
    ]
  },
  {
    "objectID": "week7.html#attribution",
    "href": "week7.html#attribution",
    "title": "8  Relevant concepts",
    "section": "8.5 Attribution",
    "text": "8.5 Attribution\nDepending on the level of LLM usage, attribution becomes a relevant concept supporting transparent implementations and decision making. Whether LLMs are used for searching information, as writing aids, or content organization, attribution becomes relevant for trust and accountability of the implementation.\nLLMs can be cited as a source of information, or attributed as a collaborator in a project. This shows the spectrum of different roles that LLMs can play in a project, and the different levels of attribution that can be used.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Relevant concepts</span>"
    ]
  },
  {
    "objectID": "week7.html#data",
    "href": "week7.html#data",
    "title": "8  Relevant concepts",
    "section": "8.6 Data",
    "text": "8.6 Data\nFinally, a relevant concept surrounding LLMs is data. Not only with the data used for training the specific model, but also with user data management.\nAwareness of training data can help manage possible output bias in either the facts used or the way information is presented. A challenge presented with LLMs is the tendency towards some sort of average voice. Many instructors worry about students loosing their writing voice.\nA practical consideration is with respect to the particular LLM data privacy policies. This becomes very relevant when sharing private information. For example, for instructors, sharing student data is very sensitive. Also, prompting information concerning research or copyrighted material could potentially be conflicting.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Relevant concepts</span>"
    ]
  },
  {
    "objectID": "week2.html",
    "href": "week2.html",
    "title": "3  Academic integrity and copyright",
    "section": "",
    "text": "3.1 Academic Integrity\nMost of the current conversations regarding LLMs in higher education environments refer to academic integrity and copyright. This is something that is increasingly in instructors’ minds and that needs careful consideration and clear communication.\nIn my experience, there is a wide range of perspectives from instructors and students about this. I will not side with any one particular approach or recommendation. However, I do believe that it is fundamental to provide clarity and not to assume that there is already a common sense about this. Artificial Intelligence usage by the wider population is very recent and we are still adjusting and defining expectations of what is acceptable from it.\nSome fields in higher education have engaged more in conversations about academic integrity than others. This is highly dependent from context, student body, and instructional preference. However, most of the discussion regarding AI in higher education has been centered at academic integrity.\nThere are different viewpoints on what is and what is not allowed in a course. My focus here will not be to determine what is permissible or not in a course, but what considerations could be effective and clear to avoid issues concerning academic integrity.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Academic integrity and copyright</span>"
    ]
  },
  {
    "objectID": "week2.html#academic-integrity",
    "href": "week2.html#academic-integrity",
    "title": "3  Academic integrity and copyright",
    "section": "",
    "text": "3.1.1 Syllabus\nA common conception for course syllabi is that they provide a type of contract between instructors and students. Similar to a user agreement, syllabi give an opportunity for instructors to clearly outline the expectations for students in a course. As such, this is a great place to clarify what is allowed or not, or expected or not with respect to LLM usage.\n\n\n\n\n\n\n\n\nFor students\nFor instructors\n\n\n\n\nMake sure to read the “AI Policy” and/or “Academic Integrity” section of your syllabus. If these are too generic or not clear, reach out to your instructor and request they clearly state to what extend they allow LLMs to be used in your class.\nInclude very detailed “AI Policy” and/or “Academic Integrity” section of your syllabus. Consider giving a few explicit examples of expected usage.\n\n\n\n\nAcademic integrity matters can be tricky. An effective way to explore what works in your particular case is to go beyond allowed v. forbidden usage and to consider to what extend can something be used.\nFor example, to completely forbid LLM (or AI) usage in a class might not be realistic nor productive. On the other hand, to completely allow unrestricted usage might be detrimental for student learning. Try to see this as a spectrum. The task then becomes to figure out where in that spectrum are you located for the specific course.\n\nSAMPLE AI POLICY\nThis is part of the AI policy that I am including in my most recent Introduction to Proofs course:\nUsage of generative AI\nThe use of generative AI in academic environments must be considered with care. A good rule to keep in mind that using generative AI to replace what a human could do might conflict with general rules of academic honesty. For our course, the recommendation is to avoid the usage of generative AI tools for assignments unless explicitly stated. When this is allowed, please make sure to cite, attribute, and/or describe to what extend you used it in your work and learning.\n\n\n\n3.1.2 Assignments\nSometimes syllabi can be a bit generic with policy and could fall short on specific considerations for paritcular assignments.\n\n\n\n\n\n\n\n\nFor students\nFor instructors\n\n\n\n\nMake sure to ask your instructor about the extend to which you can use LLMs for each assignment type. When in doubt, a general good practice is to disclose and describe how did you use LLMs.\nInclude in your assignment instructions to what extend students can use LLMs and what kind of attribution they are expected to include.\n\n\n\n\nLLMs can be thought of as a very sophisticated tool. Just as referencing Wikipedia has become normal, citing or referencing LLMs is, in general, a good practice. However, LLMs can also be thought of as more than just tools. They also act like agents, which gives them an air of autonomy and decision making. In this sense, it is also useful to think about LLMs as entities, hence the idea of attribution. This might depend on the usage.\nFor example, if an LLM was used to find a synonym, attribution might not be necessary, however if it was used to generate an example or create a summary, this might be the case.\nA good rule of thumb is to think what would be the best practice if instead using an LLM we would’ve asked a peer to do the same task. Would we had attributed their help?\n\nSome ways in which you can attribute or disclose LLM usage is by including the following, either in assignments or class material:\n\nprepared with the assistance of AI\nAI was used for generating graphics and schematics\nexample generated with the assistance of AI\n\n\n\n\n3.1.3 Community guidelines\nEach course is different. Not only due to the subject matter, but also due to everyone’s background and values. Promoting discussions regarding academic integrity considerations can also be a good team-building exercise in courses. Even more, sharing the rationale behind why advocating or discouraging certain practices can also get buy-in for both instructors and students.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Academic integrity and copyright</span>"
    ]
  },
  {
    "objectID": "week2.html#copyright-and-intellectual-property",
    "href": "week2.html#copyright-and-intellectual-property",
    "title": "3  Academic integrity and copyright",
    "section": "3.2 Copyright and intellectual property",
    "text": "3.2 Copyright and intellectual property\nLLMs have been at the center of intense debate around intellectual property. Both from the legal perspective involving companies training data, to the ownership of the outputs of these models, copyright is and will probably be a contentious topic that will follow LLMs for some time.\nIn the case of college courses, copyright becomes a consideration for students and instructors alike. Is it the student’s work if they prompted an LLM to write an essay? Is it the instructor’s work if they prompted the LLM to generate a slide deck based on their class notes?\nAs many legal scholar would reply: it depends. An important point with copyright has to do with creativity and novelty.\nIt is difficult to have a clear cut answer that applies in all cases, however the considerations below can provide some practical reflections\n\n\n\n\n\n\n\n\nFor students\nFor instructors\n\n\n\n\nTake LLM output as if another person wrote it. This is a useful rule-of-thumb that can help you decide wheter or not to disclose LLM usage. A great learning practice is to always rephrase and edit outputs in your own words the outputs of LLMs. This also helps with your own learning.\nConsider including the phrase assisted by AI when using LLM outputs in generating or preparing materials for your courses.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Academic integrity and copyright</span>"
    ]
  },
  {
    "objectID": "week1.html",
    "href": "week1.html",
    "title": "2  Practical tips and considerations",
    "section": "",
    "text": "2.1 Summaries and outlines\nAs of this moment, the most popular LLMs are:\nThese have a wide range of usage, going from free accounts to different tiers of paid subscriptions. The following use-cases are independent of the model used.\n*Llama doesn’t have a dedicated website but it is available as a companion in Meta apps, such as Facebook, Instagram, and WhatsApp.\nA powerful way to use LLMs in college is for summarizing information.\nNowadays, most LLMs have the option to take multiple input formats. You can include a combination of text, image, videos, slide decks, and others as part of your prompt.\nLLMs provide with very good summaries, however, as we will see throughout this course, there could be shortcomings such as hallucinations and context bias.\nSummarizing entails subject specific knowledge. In other words, creating a summary requires to define what is important or relevant from the given information. What is important might be a slightly subjective task.\nProviding LLMs with more context can make results more effective.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Practical tips and considerations</span>"
    ]
  },
  {
    "objectID": "week1.html#summaries-and-outlines",
    "href": "week1.html#summaries-and-outlines",
    "title": "2  Practical tips and considerations",
    "section": "",
    "text": "FOR STUDENTS\nUsing LLMs to summarize class notes.\nTry:\ngive me the most important points from these class notes [attach or copy paste notes]\n\n\nFOR INSTRUCTORS\nUsing LLMs to identify key concepts from sources.\nTry:\ngive me the five biggest ideas in this article [attach, copy paste, or include URL]\n\n\n\n\n\n\nTry:\nInstead of the simple prompts from before, try adding more context including detailed descriptions about the course, the type of resource, the audience, and the expected focus,\nThese notes are from a first year college biology course aimed at non-majors focusing in the previous weeks on cell theory, cell organelles and their functions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Practical tips and considerations</span>"
    ]
  },
  {
    "objectID": "week1.html#wikipedia",
    "href": "week1.html#wikipedia",
    "title": "2  Practical tips and considerations",
    "section": "2.2 Wikipedia++",
    "text": "2.2 Wikipedia++\nMore than summarizing given information, LLMs can also help with searching for very specific information on the web.\nThe internet is full of information about virtually any topic known to humans -to some degree-. Sometimes navigating this can be difficult. LLMs can be used for gathering information to course concepts, tools, or strategies.\n\n\n\n\n\n\n\n\nFor students\nFor instructors\n\n\n\n\nyou can prompt for information about a class topic and even for references and online resources.  Try:  what are eigenvalues for the disk?\nyou can use LLMs to search for teaching strategies, group activities, or assignment ideas.  Try: give me ideas for worksheets for eigenvalues of the laplacian in 2D domains.\n\n\n\n\nOne of the advantages of using LLMs is that they are pretty helpful even if you are not 100% sure on how to phrase your query. In this case, an edge with regular web searching is that just a vague description of a question can still provide useful information.\nMany of the outputs include references that can be used for checking the accuracy of the response and to go deeper into the topic. As usual, beware of hallucinations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Practical tips and considerations</span>"
    ]
  },
  {
    "objectID": "week1.html#translaterephrase",
    "href": "week1.html#translaterephrase",
    "title": "2  Practical tips and considerations",
    "section": "2.3 Translate/rephrase",
    "text": "2.3 Translate/rephrase\nA very useful implementation of LLMs for courses is to translate and rephrase text. This is particularly relevant for communicating ideas in more approachable language, as well as for ESL (English as Second Language) individuals.\n\n\n\n\n\n\n\n\nFor students\nFor instructors\n\n\n\n\nUse LLMs to rephrase questions or problems. You can prompt to rephrase something with less or more technical language. For ESL students, prompt LLMs to translate the questions or materials into your native language.\nUse LLMs to simplify language or to provide clarifying notes. You can also use them as a way to proofread your material and to calibrate the academic level of it.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Practical tips and considerations</span>"
    ]
  },
  {
    "objectID": "week1.html#examples-and-follow-ups",
    "href": "week1.html#examples-and-follow-ups",
    "title": "2  Practical tips and considerations",
    "section": "2.4 Examples and follow ups",
    "text": "2.4 Examples and follow ups\nAnother practical use of LLMs is the ability to generate examples of certain topics, as well as to produce follow ups, just like in a regular conversation.\n\nFOR STUDENTS\nYou can prompt to generate examples of a topic or concept.\nTry:\ngive me an example of balancing chemical equations\nWith the example provided, ask follow ups to clarify, rephrase, or modify the answer\nwhy do we need 2 H2Os on the right in step 4?\n\n\nFOR INSTRUCTORS\nGenerate ideas of possible examples to explore in class\ngive me ideas for problems involving the schrodinger equation for a sophomore level chemistry course without using differential equations.\nAs a follow up you can prompt for suggestions on the activities\nsuggest an activity in groups for 20 minutes based on problem type 4.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Practical tips and considerations</span>"
    ]
  },
  {
    "objectID": "week1.html#hallucinations",
    "href": "week1.html#hallucinations",
    "title": "2  Practical tips and considerations",
    "section": "2.5 Hallucinations",
    "text": "2.5 Hallucinations\nA common consideration when using LLMs are hallucinations. These are non-factual responses or made-up references. It is important to always fact check responses generated by any LLM.\nA good strategy is to take LLM outputs as initial drafts, starting points, or broad insights about a topic.\nNewer models are getting better with handling hallucinations, however we shouldn’t blindly trust the output of an LLMs.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Practical tips and considerations</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This course is intended to provide practical guidance and technical considerations on using LLMs in higher education teaching and learning. Large Language Models, or LLMs for short, are a type of Generative Artificial Intelligence (GenAI), which in turns is a type of Artificial Intelligence (AI). Many use these terms interchangeably when referring to LLMs, especially in educational settings. I will focus on LLMs, and will use this terminology, in order to be accurate and to center the discussion around text generation. Similar discussions can be held around image or video generation, as well as other types of AI.\nWhether you are a college student or professor, this course will help you explore effective ways to use LLMs in your daily activities. Here, we will discuss writing and coding as one of the most evident use cases, while also reflecting on effective ways to use LLMs to enhance our capacity to teach and learn.\nThis course includes typical use cases, ethical considerations, environmental concerns, privacy and data aspects, -some- technical elements, and general concepts involved in LLMs.\nAI models are rapidly evolving and so are their use cases. This course will be regularly updated in order to include current features and applications.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "week8.html",
    "href": "week8.html",
    "title": "9  Tips, tricks, and useful prompts",
    "section": "",
    "text": "9.1 Useful considerations\nFinally, here is a summary of some relevant things to keep in mind when using LLMs for teaching and learning.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tips, tricks, and useful prompts</span>"
    ]
  },
  {
    "objectID": "week8.html#useful-considerations",
    "href": "week8.html#useful-considerations",
    "title": "9  Tips, tricks, and useful prompts",
    "section": "",
    "text": "9.1.1 Give enough context\nInclude information in your prompt about your role and the purpose of your request.\ni am teaching an upper division course for math majors in an undergraduate institution. check the following problems for clarity in language. [attach or copy-paste the problem statments]\n\n\n9.1.2 Follow-ups\nJust as with writing, LLMs provide better results with edits/follow-ups. Instead of crafting a comprehensive single prompt, ask improvements or clarification with follow-ups.\n\n\n9.1.3 Feedback and rephrasing\nAsk LLMs to give feedback on a given text. You can take it a step further by assigning the LLM an editorial role:\nfor the text below, give me feedback as if you were an editor. focus on clarity of the text, as well as the language used. this is for a lower division chemistry course. check consistency, grammar, and technical language.\n\n\n9.1.4 Summaries and outlines*\nLLMs can be very useful for summarizing text and identifying important elements. However, summarizing a text involves identifying important elements in it. This identification can be very contextual, and course dependent. The same text could have multiple perspectives of what is more relevant depending on the particular focus of a given course or instructor. A particular focus could be included in the prompt in order to improve the usefulness of the output.\n\n\n9.1.5 Uncovered reasoning\nA good way to improve the quality of LLM outputs is by prompting it to explain it’s reasoning. When used for solving problems or analyzing situations, including to explain it’s reasoning at the end usually leads to better results.\n\n\n9.1.6 Tutor/disciple (for learners)\nYou can endow LLMs with (temporary) roles. Use it as a tutor if you want it to help you clarify concepts related to a class:\ni am a sophomore student taking BIO 20 at UCSC. you are a biology tutor helping me with my homework. i will ask you some questions related to concepts that are not very clear to me. help me understand them and guide me through these homework questions.\nA very effective way to learn a topic is by teaching it. Consider also endowing the LLMs with a disciple role. You can prompt it to forget everything it knows about a topic and that you will explain it.\nforget everything you know about projectile motion. i will explain you the important concepts related to this. ask me clarifying questions when something is not clear.\n\n\n9.1.7 Test student (for instructors)\nYou can give the LLM the role of a test student in your class and ask it to check an assignment for clarity and level.\nyou are a student in my calculus 1 class. at this moment, we are covering the chain rule. check the following assignment for clarity (for first year students in STEM) and estimate the level of the problems.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tips, tricks, and useful prompts</span>"
    ]
  },
  {
    "objectID": "week8.html#careful-considerations",
    "href": "week8.html#careful-considerations",
    "title": "9  Tips, tricks, and useful prompts",
    "section": "9.2 Careful considerations",
    "text": "9.2 Careful considerations\nLikewise, there are some uses that can be problematic, not ideal, or even dangerous for teaching and learning.\n\n9.2.1 Writing essays or solving problems (learners)\nLLMs are becoming better at these tasks, however careful considerations needs to be made here. As a learner, one of the main goals of writing an essay or solving a question/problem is the process of doing it yourself. Learning happens in thinking about how to structure the essay, on how to approach the question or problem, and the trail and error to finally achieve the goal. Generating essays or solutions skips the learning process altogether.\n\n\n9.2.2 Grading (instructors)\nEven more when LLMs behave as a black-box, using them for high-stakes tasks such as grading assignments can be highly inaccurate. For classification type tasks (such as grading), LLMs are biased towards their training data. If this is not clear, the results can be very unfair and inconsistent. Besides this, using LLMs for grading can be problematic due to privacy issues when sharing student data.\n\n\n9.2.3 Hallucinations\nIn general, LLMs are stochastic models that produce text. As such, they are prone to generating non-factual outputs. This issue is getting better, however, it is important to always double check the outputs and not to take them as 100% true.\n\n\n9.2.4 Calculations\nLLMs are language models and in general, do not have computational engines. As such, they are not optimal for performing calculations, such as arithmetic or calculus. It is much better to use a calculator instead, as the results are reliable and reproducible.\n\n\n9.2.5 Short answers\nPrompting LLMs to produce short answers (some times a single word) can increase the chances of hallucinations. One of the best strategies to improve accuracy in LLMs is to have them explain their reasoning. Going the opposite direction and prompting to only reply with a short answer can produce noise in the output.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tips, tricks, and useful prompts</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Summary\nIn summary, this book has no content whatsoever."
  },
  {
    "objectID": "week3.html",
    "href": "week3.html",
    "title": "4  What are LLMs anyway?",
    "section": "",
    "text": "4.1 Predictive text\nAs mentioned before, there are terms that are used more or less interchangeably when talking about LLMs in higher education. Sometimes people refer to them as “AI” or “Artificial Intelligence.” Other times, people refer to this as “genAI” or “generative Artificial Intelligence.” Usually, these terms are made in reference to LLMs or “large language models,” which are a type of genAI, which is a type of AI.\nI am not going to describe the technical aspects of LLMs here, but we will explore the big features and components of LLMs to understand better their reach and limitations, as well as to value their outputs.\nLarge Language Models specialize in generating text following a prompt. This text generation is based on a combination of mathematical -and statistical- models that are trained on data, loads and loads of data. But before diving into the moving pieces of LLMs, let’s check a predecessor of modern day LLMs.\nWe all have used auto-correct or some sort of predictive text assistance in our phones, email, or even your favorite text editor. These tools have become ubiquitous across devices and platforms.\nOne of the early versions of predictive text is based on a mathematical and statistical concept called Markov chains. In short, a system presents the Markov property if its current state is determined only by its previous state. Translated into text prediction, this means that suggesting the next word in a text only requires knowledge of the current (last) word.\nThis property might seem very simplistic and perhaps not useful. However, it is impressive the amount of modern day technology that is based on this simple assumption. The Markov property makes systems to be lightweight, meaning that not a lot of resources are needed in order to implement them.\nRoughly speaking, Markov chains describe systems that have a finite number of possible states. These systems transition from one state to another in each time step. It is allowed to stay in the current state. The transitions occur according to certain probabilities. Much of the practical work focuses on accurately describing and computing these probabilities.\nFor example, for our simple predictive text model, the system described is a text (essay, sms, email, etc.) as a sequence of words. Our Markov chain aims to describe the transition from one word to the next one. For this, we need to know the probability of going from one specific word to the next one. If in my text there is the word “I,” there is a high probability that the next word is “am” and a low probability that the next word would be “I” again.\nComing up with these probabilities can be challenging and not standard. Akin to different chefs having different recipes for the same dish, different implementations of Markov chains might compute or estimate probabilities in different ways.\nA common way to estimate probabilities for predictive text (as before) is to use frequency counts on reference corpora (reference texts). The main idea is to tally how frequent the word “am” follows the word “I” in a collection of reference texts and to compare it to the total number of pairs of words that have “I” as the first word.\nNotice that this depends on the reference corpora. If our reference text were the lyrics of Avril Lavigne’s song “I’m with you,” the word “am” doesn’t show up as a possible follow-up to the word “I,” however the word “I” does!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>What are LLMs anyway?</span>"
    ]
  },
  {
    "objectID": "week3.html#predictive-text",
    "href": "week3.html#predictive-text",
    "title": "What are LLMs anyway?",
    "section": "",
    "text": "We all have used auto-correct or some sort of predictive text assistance in our phones, email, or even your favorite text editor. These tools have become ubiquitous across devices and platforms.\nOne of the early versions of predictive text is based on a mathematical and statistical concept called Markov chains. In short, a system presents the Markov property if its current state is determined only by its previous state. Translated into text prediction, this means that suggesting the next word in a text only requires knowledge of the current (last) word.\nThis property might seem very simplistic and perhaps not useful. However, it is impressive the amount of modern day technology that is based on this simple assumption. The Markov property makes systems to be lightweight, meaning that not a lot of resources are needed in order to implement them.\nRoughly speaking, Markov chains describe systems that have a finite number of possible states. These systems transition from one state to another in each time step. It is allowed to stay in the current state. The transitions occur according to certain probabilities. Much of the practical work focuses on accurately describing and computing these probabilities.\nFor example, for our simple predictive text model, the system described is a text (essay, sms, email, etc.) as a sequence of words. Our Markov chain aims to describe the transition from one word to the next one. For this, we need to know the probability of going from one specific word to the next one. If in my text there is the word “I,” there is a high probability that the next word is “am” and a low probability that the next word would be “I” again.\nComing up with these probabilities can be challenging and not standard. Akin to different chefs having different recipes for the same dish, different implementations of Markov chains might compute or estimate probabilities in different ways.\nA common way to estimate probabilities for predictive text (as before) is to use frequency counts on reference corpora (reference texts). The main idea is to tally how frequent the word “am” follows the word “I” in a collection of reference texts and to compare it to the total number of pairs of words that have “I” as the first word.\nNotice that this depends on the reference corpora. If our reference text were the lyrics of Avril Lavigne’s song “I’m with you,” the word “am” doesn’t show up as a possible follow-up to the word “I,” however the word “I” does!"
  },
  {
    "objectID": "week3.html#knowledge-and-prediction",
    "href": "week3.html#knowledge-and-prediction",
    "title": "4  What are LLMs anyway?",
    "section": "4.2 Knowledge and prediction",
    "text": "4.2 Knowledge and prediction\nJust relying on the previous word to predict the next word can feel overly simplistic. Notice that the same idea of Markov chains can be applied for bi-grams (pairs of words) instead of single words. We can estimate probabilities of what is the next word given the two previous words. In general, it is possible to extend this idea to n-grams, sequences of \\(n\\) words, as the input for predicting the next word.\nAs we would expect, taking more words than just the previous one leads to better results in predictive text. The price to pay is not only now keeping track of a longer sequence of words for predictions, but also considering more possible combinations when estimating the transition probabilities.\nEnlarging the context window for a predictive system requires more attention to the estimation of the transition probabilities. These probabilities can then be thought of as the knowledge that the system has with respect to certain corpora. The predictive system emerges with two important components: a) the knowledge it has, and b) the capacity to predict the next word given an input.\nEach of these components have different strategies and algorithms involved which can differ from implementation to implementation, but the essence is roughly the same:\n\nKnowledge is represented by defining probabilities.\nPrediction is obtained by input sequences of words, interpreting probabilities, and introducing random choices.\n\nNotice that probability and randomness are important parts of the system. There are some theoretical and practical reasons for this, but one of the main points for us is the intrinsic stochastic nature of predictive text.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>What are LLMs anyway?</span>"
    ]
  },
  {
    "objectID": "week3.html#large-language-models",
    "href": "week3.html#large-language-models",
    "title": "4  What are LLMs anyway?",
    "section": "4.3 Large Language Models",
    "text": "4.3 Large Language Models\nNotice that the above description doesn’t take into account the language’s grammar. This approach infers the structure of the language from the reference corpora -also known as the training data.\nThere are different language models used by linguists and computer scientists which have strengths and weaknesses depending on their use cases. It is important to notice that LLMs are a special type of language models that arose as a type of improvement from our simple predictive text system.\nBesides the technical mathematical differences, we can focus on how LLMs define their knowledge and predict the next words.\nAs opposed to our predictive text system from before, LLMs are called large language models due to the amount of training data. Before, we could have the lyrics of a song, a few Wikipedia pages, or a book as possible corpora for estimating transition probabilities. In the case of LLMs, training data reaches the level of the entire internet. Even beyond this, many of the current LLMs are trained on data sets including all digitized books, music, movies, etc. There have been multiple copyright lawsuits addressing the unauthorized usage of copyrighted material in training some of the biggest models.\nOn the prediction side, one of the most relevant differences from our example before is the dynamic context window used for predicting words. The main principle remains the same: given a sequence of words, what is the most likely word to follow. However, LLMs take the entire word sequence given in a prompt as opposed to just a fixed context window. This remains true when prompting follow-ups. Now, the LLMs doesn’t only take the new request as the context window, but also the initial prompt, together with the output it itself produced about it.\n\nA technical note: LLMs are trained not on words but on tokens. These can be words, punctuation marks, mathematical symbols, coding symbols and instructions, etc. For example, when writing “7x8=” the LLM will predict that the most probable follow up is a “5” and then a “6”.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>What are LLMs anyway?</span>"
    ]
  },
  {
    "objectID": "week3.html#training",
    "href": "week3.html#training",
    "title": "4  What are LLMs anyway?",
    "section": "4.4 Training",
    "text": "4.4 Training\nUsing the entire internet to train an LLM requires, not only a lot of data, but also a lot of computing. Even though the specific mathematical details of training an LLM and estimating transition probabilities in a Markov chain are different, the core principle remains the same: we need to represent knowledge on how to predict the next word given a sequence of previous words. This knowledge is represented by numbers which will be stored and are known as the parameters of the model. At the time of writing this document, current models range between a few tenths of billions to over a trillion parameters (\\(10^9-10^{12}\\)).\nAll of these numbers are the result of solving or estimating mathematical equations based on the tokenization of the training data. This means to first clean and break the training data into a sequence of tokens. In order to incorporate mathematical models based on these tokens, it is needed to represent these tokens as numbers -more specifically as vectors-, which can be thought of as arrays of numbers.\nThis process is called encoding. We can think of encoding as a mathematical dictionary that equates each token with a different vector. But how is this dictionary built?\nThe challenge is to make this dictionary as useful as possible. This means that these numbers should represent tokens and how they interact with each other in a useful way. An initial strategy addressing this problem was to have a universal dictionary that can be used for every type of language model. However, a practical issue occurs with polysemic words such as “Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo.” Here, humans are able to understand the meaning of such sentence due to the word buffalo changing meaning depending on the position in the sentence. In order to allow for this flexibility, LLMs don’t assume a given encoding for a word, but compute the encoding for each word in the sequence, allowing it to be different at different places. This allows for the context to be updated as more information is included in the text.\nThis is important not only for words that can have multiple meanings, but also that can change meaning depending on other words:\n\n\nYeah!\nYeah, right.\n\n\nThe ability to change the encoding of tokens depending on the context is referred to as auto-encoding. This adds to the computation needed for training LLMs.\nAfter the auto-encoding stage, most LLMs have a combination of neural networks that are used for computing possible next tokens. In simple terms, neural networks are mathematical functions that depend on certain parameters. The parameters are determined by minimizing the error between the predicted tokens and the actual tokens present in the training data.\nThese are highly intensive computational tasks due to the number of parameters (between \\(10^9\\) and \\(10^{12}\\)). For this, several computers (data centers) are used where the training process takes a few months of non-stop computations.\nAt this moment, some of the newest models took between 1.5 to 3 months of training, using between 10,000 and 25,000 GPUs (graphic processing unit, which are specially efficient at matrix multiplication), spending an estimated 5GWh - 60GWh of power. For reference, this would be the equivalent to the annual electricity consumption of a small town with 50,000 homes.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>What are LLMs anyway?</span>"
    ]
  },
  {
    "objectID": "week6.html",
    "href": "week6.html",
    "title": "7  Best practices",
    "section": "",
    "text": "7.1 Context\nThere are multiple opinions regarding the role of LLMs in education. Regardless on the extend in which you are using Large Language Models for your own teaching and learning, there are certain things to keep in mind in order to enhance -and not replace- your thinking.\nOne of the most critical challenges of LLM outputs is that they can be too generic -and possibly not very helpful. Providing enough context about the task is important for making effective prompts and conversations.\nFine tunning often means to give a specific context for every single prompt. Consider saving this basic context as a reference file in the LLM or even as a text note that you can copy and paste every time you prompt something related to a particular class.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Best practices</span>"
    ]
  },
  {
    "objectID": "week6.html#context",
    "href": "week6.html#context",
    "title": "7  Best practices",
    "section": "",
    "text": "Consider describing the background of the task: what is it for? which course is it for? what is the expected level (frosh, senior, etc.) what is your goal with it?\nEndow the LLM with a character or a role for the task: you are a tutor for this course you are a course assistant you are an editor providing feedback you are a student in this course\nFine tune the LLM for language or knowledge expertise: think about limiting the knowledge or references to the current course or any other prior course. Including a syllabus or course program can help give more context.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Best practices</span>"
    ]
  },
  {
    "objectID": "week6.html#evaluation",
    "href": "week6.html#evaluation",
    "title": "7  Best practices",
    "section": "7.2 Evaluation",
    "text": "7.2 Evaluation\nFront loading work in prompts is not the only way in which we can make our interactions with LLMs more effective. It is important to analyze their outputs and evaluate if they are achieving our expectations.\nSince LLMs have some level of agency -they make tiny decisions as in what text to produce- it is useful to evaluate them as a supervisor would do with an assistant.\nFor this, it is important to have key rubric items that we can focus on while evaluating the LLM’s outputs:\n\n\n\n\n\n\n\nItem\nDescription\n\n\n\n\nCompliance\nDid the LLM generate what you were expecting?\n\n\nHallucinations\nAre the facts used real?\n\n\nData\nDid the LLM used the appropriate data or references?\n\n\nVoice\nIs the output given in the right voice, language, and terminology?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Best practices</span>"
    ]
  },
  {
    "objectID": "week6.html#trust-score",
    "href": "week6.html#trust-score",
    "title": "7  Best practices",
    "section": "7.3 TRUST Score",
    "text": "7.3 TRUST Score\nOne of the most important aspects of LLM implementations is trust. This involves different levels. From the architecture of the model, the data used for training, to the way it produces outputs, and the usefulness/truthfulness of the responses, it is important to have a holistic sense of trust for the implementation.\nThere are three stages that are relevant for this:\n\nTraining\nProcessing\nEvaluation\n\nA simple way to assess your awareness of your implementation is by computing what I call the TRUST score: Transparency, Risk, Usefulness, Safety, and Trust. Each stage has two components and each component has a maximum total points that can be assigned. These are self-assigned points that can help you assess your own knowledge and awareness of the whole LLM system and implementation that you are using.\n\n\n\n\n\n\n\n\nStage\nDimension\nDescription\n\n\n\n\nTraining\nData (1 pt)\nEvaluate the data sources, data quality, copyright and privacy of the data used for training.\n\n\nTraining\nFootprint (1 pt)\nConsider the environmental and labor impact of the LLM.\n\n\nProcessing\nExplainability (3 pts)\nHow well does the user understand the output.\n\n\nProcessing\nPrivacy (3 pts)\nAssesses data ownership, storage, and usage practices.\n\n\nEvaluation\nAssessment (9 pts)\nGauges the effectiveness and accuracy of the outputs.\n\n\nEvaluation\nAccountability (9 pts)\nEnsures clear human responsibility at every stage of the process.\n\n\n\nThe total TRUST possible score is 26 points.\nThis score can help guide to what extend you need to consider a different LLM, consider different tasks to be outsourced, or to what extend you can learn more about the LLM system itself.\nHere are some recommended actions based on the TRUST score that you obtain after your self-assessment:\n\n\n\n\n\n\n\n\nLevel\nScore\nAction\n\n\n\n\nHigh TRUST\n&gt; 18 pts\nImplement the system while periodically reassessing if any dimensions have changed.\n\n\nModerate TRUST\nBetween 13-18 pts\nIdentify and address specific weaknesses in the lowest-scoring stages Reassess before implementing the system.\n\n\nLow TRUST\nBetween 5-12 pts\nReview all stages of the system (training, processing, implementation) for compliance with current policies and regulations.\n\n\nMinimal TRUST\n&lt; 5pts\nConsider a system redesign and/or explore a different system. Consult with supervisors and search for alternatives.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Best practices</span>"
    ]
  },
  {
    "objectID": "week5.html",
    "href": "week5.html",
    "title": "6  Limitations",
    "section": "",
    "text": "6.1 Context windows\nLarge Language Models have limitations depending on the usage and context. Some of these limitations are due to technological barriers, others due to their design.\nBy design, most LLMs have a user interface that resemble a chatbot. This design fosters conversation-like interactions rather than just simple query/response.\nOne of the most important advances in LLMs is due to the so-called “attention mechanism.” This neural network architecture allows the model to not only to pay attention to the most recent words, but also to all the text from the beginning.\nAn example of this is how predictive text in cellphones seem to only provide suggestions based in the last 1 or 2 words typed. This is not ideal since the model doesn’t have memory of anything said earlier in the conversation. In order to address this, the attention mechanism incorporates a dynamic encoding of words that captures the evolution of the text.\nIn practice when using an LLM, this bigger attention span consumes resources and it often is limited to a particular number of context words or context window. This context window can be thought of as the background information that is passed to the model in order to provide specific outputs related to our conversations.\nThis context window is also used for fine tunning the model. That is, to tell the model the specific expertise, language, or character that should assume.\nLLM companies usually have different tiers of models that allow, among other things, to select bigger context windows. This becomes relevant when conversations become long or when reference documents are lengthy.\nUsually LLM chatbots append previous outputs as part of the input for new prompts. In this way, the previous information and context is passed to the new queries and the chatbots are able to remember what has happened in the conversation.\nSimilarly, reference documents are passed to the LLM as part of the context, hence the limitation on the number of active references that a conversation can handle.\nContext windows can be very small in some of the free tiers for commercial LLMs. This is an important aspect to have in mind, since paid users might have an advantage over free users in the type of tasks that they can effectively complete.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Limitations</span>"
    ]
  },
  {
    "objectID": "week5.html#context-windows",
    "href": "week5.html#context-windows",
    "title": "Limitations",
    "section": "",
    "text": "By design, most LLMs have a user interface that resemble a chatbot. This design fosters conversation-like interactions rather than just simple query/response.\nOne of the most important advances in LLMs is due to the so-called “attention mechanism.” This neural network architecture allows the model to not only to pay attention to the most recent words, but also to all the text from the beginning.\nAn example of this is how predictive text in cellphones seem to only provide suggestions based in the last 1 or 2 words typed. This is not ideal since the model doesn’t have memory of anything said earlier in the conversation. In order to address this, the attention mechanism incorporates a dynamic encoding of words that captures the evolution of the text.\nIn practice when using an LLM, this bigger attention span consumes resources and it often is limited to a particular number of context words or context window. This context window can be thought of as the background information that is passed to the model in order to provide specific outputs related to our conversations.\nThis context window is also used for fine tunning the model. That is, to tell the model the specific expertise, language, or character that should assume.\nLLM companies usually have different tiers of models that allow, among other things, to select bigger context windows. This becomes relevant when conversations become long or when reference documents are lengthy.\nUsually LLM chatbots append previous outputs as part of the input for new prompts. In this way, the previous information and context is passed to the new queries and the chatbots are able to remember what has happened in the conversation.\nSimilarly, reference documents are passed to the LLM as part of the context, hence the limitation on the number of active references that a conversation can handle.\nContext windows can be very small in some of the free tiers for commercial LLMs. This is an important aspect to have in mind, since paid users might have an advantage over free users in the type of tasks that they can effectively complete."
  },
  {
    "objectID": "week5.html#data-bias",
    "href": "week5.html#data-bias",
    "title": "6  Limitations",
    "section": "6.2 Data bias",
    "text": "6.2 Data bias\nLarge Language Models are trained using big amounts of data. This means that they often include different perspectives or views about something. Different training algorithms face this challenge in various ways, either by ranking the information or by providing some sort of average description.\nThis limitation influences the type of responses that LLMs are able to provide, usually providing responses that, in some sense, average the information, being less sensitive for outliers or less common sources.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Limitations</span>"
    ]
  },
  {
    "objectID": "week5.html#complex-tasks",
    "href": "week5.html#complex-tasks",
    "title": "6  Limitations",
    "section": "6.3 Complex tasks",
    "text": "6.3 Complex tasks\nThere has been great progress in the so-called reasoning feature of LLMs. This usually involves a combination of back-and-forth internal interactions of the LLMs, together with explicit planning and step-by-step strategies about the LLM’s course of action. This is particularly useful for minimizing hallucinations, however when tasks are very complex and/or involve multiple steps, LLMs tend to to reduce in performance. Currently, this threshold is more noticeable for tasks that require one hour or more of processing.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Limitations</span>"
    ]
  },
  {
    "objectID": "week5.html#fast-forward-v.-black-box",
    "href": "week5.html#fast-forward-v.-black-box",
    "title": "6  Limitations",
    "section": "6.4 Fast-forward v. black-box",
    "text": "6.4 Fast-forward v. black-box\nLarge Language Models are very useful at fast-forwarding tasks. Even at the corporate level, users report that using LLMs can help them accomplish tasks in only 20% of the normal time. This approach enhances the ability of users to do what they know already, but faster.\nOn the other hand, LLMs also enable a black-box approach, where users have no prior knowledge of a field or task. Here, the models are replacing or outsourcing the human intervention.\nThese two approaches could be both useful or dangerous for teaching and learning purposes. It depends on the specific goals, usage, and context. However, it is important to fully understand in which sense instructors and students are using LLMs, whether it is to fast-forward a task or as a black-box.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Limitations</span>"
    ]
  },
  {
    "objectID": "week5.html#vibe-learning",
    "href": "week5.html#vibe-learning",
    "title": "6  Limitations",
    "section": "6.5 Vibe-learning",
    "text": "6.5 Vibe-learning\nBeyond the black-box approach is what I call vibe-learning. In early 2025 the term vibe-coding was coined to describe the way in which many developers and computer scientists were using LLMs by “focusing on the goal of what needs to be accomplished and forgetting about the syntax of coding.” In this sense, vibe-coding enables developers to forget about the small details and focus on the big picture of the projects they are pursuing.\nIn general, LLMs can be used with this vib-ing approach. Although, a word of caution is relevant in the teaching and learning setting. One important difference between novices and experts in a field is the attention to details. Experts tend to think more on the big picture and overarching themes, while novices focus on small details. This attention to details is fundamental for the learning process in any field. The overuse -or misuse- of LLMs in teaching and learning environments can hinder students’ ability to effectively learn concepts by skipping or reducing their attention to details.\nSince LLMs can output products regardless of the users expertise or knowledge of a subject area, this usage could falsely lead users to feel they are engaging in learning. If there is anything more dangerous than ignorance is the illusion of knowledge.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Limitations</span>"
    ]
  }
]